
---
title: "Recap of network exploration"
author: "SC"
date: "`r Sys.time()`"
output: 
    bookdown::html_document2:
        code_folding: hide
        section_numbers: true
        toc: true
        toc_depth: 3
        toc_float: true
        fig_width: 10
        fig_height: 8

---

<style type="text/css">
body .main-container {
max-width: 1800px;
margin-left: auto;
margin-right: auto;
}
p.caption {
  font-size: 0.8em;
  font-weight: bold;
}

</style>

# Intro


I'll try recap what we did and what I think can be the next step. I will use a "frozen" version of the dataset, this way it will be possible to understand any difference in future analysis.

## Data processing 



```{r loadDataAndPackages ,results="asis"}
knitr::opts_chunk$set(warning=FALSE,message=FALSE,cache=FALSE,collapse=TRUE,fig.show="hold",results="hide",fig.align="center")

#the frozen dataset

isotopes=read.csv("data/DATA isotopie_Pryce_V13092022.csv")
knitr::kable(head(isotopes))

## useful tools, packages and shapefiles
source("tools.R")
library(sf)
seashapefile=st_read("Cartes/allseaborder_lowq.shp")
seashapefile=st_geometry(seashapefile)
ratioiso=c("X208Pb.204Pb","X207Pb.204Pb","X206Pb.204Pb")

#we introduce new shorter label, easier to read and check:
lbls=sapply(lapply(strsplit(isotopes$LABEL.analytical,"/"),"[",3:4),paste0,collapse="/")
isotopes$LABELS.shorts=lbls
```

Note that a couple of operation have been applied on the original dataset to get rid of some minor problem such as the french ","  as decimal marker and thus the number seen as character etc... here a list of the command to do so ; useful to redo the analysis from an updated dataset:

```{r cleaningData,eval=T}
#remove coma
isotopes[,ratioiso]=apply(isotopes[,ratioiso],2,gsub,pattern=",",replace=".")
isotopes[,ratioiso]=apply(isotopes[,ratioiso],2,as.numeric)
#add lead level, need the elements dataset
    #teneurs=datappm$Pb
    #names(teneurs)=datappm$LABEL.analytical
    #isotopes$teneur=teneurs[isotopes$LABEL.analytical]
    #isotopes$leaded=0
    #isotopes$leaded[isotopes$teneur>.1]=1
```




## General graph:

Now that data is cleaned, we can have a look at general properties, checking only clean sample from sites with multiples samples ( 10 and more):

```{r cleanBiplot, out.width="50%",fig.cap="Biplot for only clean samples"}
cleanlist=isotopes[isotopes$Corrosion.products %in% c("Low","Ingot","Slag"),]
persite=table(cleanlist$Site.abreviation) #count number of sample per sites, we just show main side
cleanlist=isotopes[isotopes$Site.abreviation %in% names(persite)[persite>9] & isotopes$Corrosion.products %in% c("Low","Ingot","Slag"),]
sitecol=rainbow(length(unique(cleanlist$Site.abreviation)),alpha=.8)
names(sitecol)=unique(cleanlist$Site.abreviation)
plot(cleanlist[,ratioiso[3:2]],bg=sitecol[cleanlist$Site.abreviation],pch=20+as.numeric(as.factor(cleanlist$Country)))
plot(cleanlist[,ratioiso[c(3,1)]],bg=sitecol[cleanlist$Site.abreviation],pch=20+as.numeric(as.factor(cleanlist$Country)))
val=legend("topleft",fill=sitecol,legend=names(sitecol),title="Site",bty="n")
newxleg=val$rect$left+val$rect$w
newyleg=val$rect$top
legend(x=newxleg,y=newyleg,pch=20+1:length(unique(cleanlist$Country)),legend=levels(as.factor(cleanlist$Country)),title="Country",bty="n")
```

If we look at the whole dataset, just for fun, it becomes... a bit messy
```{r cleanBiplotWholemess, out.width="45%",fig.cap="Biplot for all the dataset"}
sitecol=rainbow(length(unique(isotopes$Site.abreviation)),alpha=.8)
names(sitecol)=unique(isotopes$Site.abreviation)
plot(isotopes[,ratioiso[3:2]],col=sitecol[isotopes$Site.abreviation],pch=as.numeric(as.factor(isotopes$Country)))
plot(isotopes[,ratioiso[c(3,1)]],col=sitecol[isotopes$Site.abreviation],pch=as.numeric(as.factor(isotopes$Country)))
val=legend("bottom",fill=sitecol,legend=names(sitecol),title="Site",bty="n",ncol=10,cex=.4)
newxleg=val$rect$left+val$rect$w
newyleg=val$rect$top
legend(x=newxleg,y=newyleg,pch=1:length(unique(isotopes$Country)),legend=levels(as.factor(isotopes$Country)),title="Country",bty="n",cex=.6)
```


# Consistency check

The first level of reflection we had on Monday afternoon was: how to automatically asses the "consistency" of two sample ; _ie_ how can we confidently say the isotopic composition of a two samples from the dataset are the same?

Oli's intuition is the following:

If we look at isotopoic composition of a well defined group of samples (_ie_ samples very likely to come from the same source), the largest difference between the sample of this group should be the upper limit that should not be passed. In other world, if we take the difference between the lowest and the largest value for each isotopic value, its gives us an expected limit within two different sample can be said as consistent. The group Oli is confident about its consistency is the KWPV, with smaple from sites NKH and NPW, from which we remove a few 'weird' sample (circled on the graph below)


```{r KWPVcomplex,out.width="45%",fig.cap="Biplot KWPV group,circle outliers that will be reomved from analysis"}
KWPV=isotopes[ isotopes$Site.abreviation %in% c("NKH","NPW"),]
sitecol=rainbow(length(unique(KWPV$Site.abreviation)),alpha=.8)
names(sitecol)=unique(KWPV$Site.abreviation)
plot(KWPV[,ratioiso[3:2]],bg=sitecol[KWPV$Site.abreviation],pch=20+as.numeric(as.factor(KWPV$Site.abreviation)))
points(KWPV[KWPV$LABEL.analytical %in%  c("SEALIP/TH/NPW/11","SEALIP/TH/NPW/1"),ratioiso[c(3,2)]],col="red",cex=5,lwd=5)
plot(KWPV[,ratioiso[c(3,1)]],bg=sitecol[KWPV$Site.abreviation],pch=20+as.numeric(as.factor(KWPV$Site.abreviation)))
points(KWPV[KWPV$LABEL.analytical %in%  c("SEALIP/TH/NPW/11","SEALIP/TH/NPW/1"),ratioiso[c(3,1)]],col="red",cex=5,lwd=5)
val=legend("center",fill=sitecol,legend=names(sitecol),title="Site",bty="n",ncol=1,cex=1)
KWPV=KWPV[!(KWPV$LABEL.analytical %in%  c("SEALIP/TH/NPW/11","SEALIP/TH/NPW/1")),]
```

From this, Oli's approach, translated in R term, goes as follows:


1. We the minimal and maximal value for each ratio
```{r}
allratio= apply(KWPV[,ratioiso],2,range)
```
2. we divid the minimal value by the maximum
```{r}
alldist=apply(allratio,2,function(i)i[1]/i[2])
```
3. take 1-ratio : if A/B is the ratio between sample A and B ; then 1-A/B will give use the proportional difference between B and A
```{r,results='asis'}
threshold=1-alldist
knitr::kable(t(threshold))
```

This is a very conservative approach: among all distances observed in the group KWPV, we take only the extreme one as a threshold. This  will be very sensitive to outliers: as if just one  value is out for one isotopes, it will be used against the smallest one to compute the threshold. 

Another approach is to keep the full distribution of distance of all samples together ; and use a interval containing most of our sample as a threshold. 
As an example let's look at just one ratio: `r ratioiso[1]` . 
```{r rankKWPV,fig.cap="ranked vlaue of 208Pb/204pb for all sample of group KWPV",out.width="50%"}

ex=ratioiso[1]
v_ex=KWPV[,ex]
plot(sort(v_ex),xlab="rank",ylab=ex)

```

for all value of `v_ex` we want to know how far it is from the bigger values. When comparing two values A and B, we only look at how many percent of the bigger the smaller is. (1-A/B gives us how many % of B A need to be equal to B. If A>B then this percentage is negative, meaning we need to reduce A to reach B ; but this isn't needed as it will be covered by 1-B/A. Thus we remove all values <=0 ; 0 being the case when A=B

```{r dist208024,fig.cap="Distribution of distance between each two pair of sample within group KWPV for ration 208/204Pb. Blue lines show specific quantiles below which a certain percentage of the population is included.",out.width="50%"}
alld=1-sapply(v_ex,function(i)i/v_ex)
alld=alld[alld>0]
plot(density(alld,from=0),main="distance all sample KWPV")
abline(v=threshold[ex],lty=2,col="red")
abline(v=quantile(alld,probs=c(.75,.85,.95)),lty=3,col="blue")
mtext(paste0(c(75,85,95),"%"),3,0,at=quantile(alld,probs=c(.75,.85,.95)),col="blue")
mtext("oli's initial values",3,0,at=threshold[ex],col="red")
```

In the previous graph we can see that 75% of the difference measured in our sample are below `r round(quantile(alld,probs=c(.75)),4)[[1]]` ; which is way below the threshold computed with the maximum range:  `r round(threshold[ex],4)`. More over, if we add to `v_ex` that is a bit off, bigger than the other while still "plausible; what happend to the different metrics?

```{r,results='asis',fig.cap="Rank (left) and distribution of distance between each two pair of sample within group KWPV(right) ,adding a outlier.",out.width="50%"}

v_ex=c(v_ex,38.21)
plot(sort(v_ex),xlab="rank",ylab=ex)
t=range(v_ex)
t=1-t[1]/t[2]
alld=1-sapply(v_ex,function(i)i/v_ex)
alld=alld[alld>0]
plot(density(alld,from=0),main="distance all sample KWPV")
abline(v=t,lty=2,col="red")
abline(v=quantile(alld,probs=c(.75,.85,.95)),lty=3,col="blue")
mtext(paste0(c(75,85,95),"%"),3,0,at=quantile(alld,probs=c(.75,.85,.95)),col="blue")
mtext("'à là oli's new estimation",3,0,at=t,col="red")
knitr::kable(t(round(c("'à là oli'-estimates"=t,quantile(alld,probs=c(.75,.85,.95))),4)))
```

The  old estimate get more thant doubled ; while the 75% interval doesn't change.

This will gives us a estimate taht, thought less conservative, will allow some flexibility about the extreme observed value (_ie_ we integrate the fact that we may have some error in the extreme value of our group KWPV).

We can then do that for the three ratios:

```{r,out.width="33%",fig.cap="Distribution of distance between each two pair of sample within group KWPV for all ratios 208/204Pb.",results="asis"}

for(r in ratioiso){
    v_ex=KWPV[,r]
    alld=1-sapply(v_ex,function(i)i/v_ex)
    alld=alld[alld>0]
    plot(density(alld,from=0),main=r)
    abline(v=threshold[r],lty=2,col="red")
    abline(v=quantile(alld,probs=c(.75,.85,.95)),lty=3,col="blue")
    mtext(paste0(c(75,85,95),"%"),3,0,at=quantile(alld,probs=c(.75,.85,.95)),col="blue")
    mtext("oli's initial values",3,0,at=threshold[r],col="red")
}

threshold  <- sapply(ratioiso,function(r){
                     v_ex=KWPV[,r]
                     alld=1-sapply(v_ex,function(i)i/v_ex)
                     quantile(alld[alld>0],probs=.75)[[1]]
})
knitr::kable(t(threshold))
```

  One needs to notes that in theory (and in practice), Oli's estimations compute the biggest difference within the sample of the group studied. Thus, the red vertical lines should be _always_ at the end of the curve. It is not the case because the kernel estimation "invent" data after the end of the measurement to smooth the curve. In this version we will still use the old version of the threshold, though I have a problem with it it doesn't give me exactly the same result than before.  Maybe because i miss some data?


```{r}

#threshold=1-alldist
```


## Group together consistent samples

The next step is to apply this method to all sample and see if it groups them in meaningful clusters. There are a huge range of method to do such a grouping, bu given the threshold established before that gives us a binary response (consistent - 1, or not - 0), and following Jelana and Miljana's method, we will 

1. test all artefact pairwsie and compute a matrix of consistency
2. use this matrix as a networ: each 1 represent a link between two artifacts.

### Myanmar check

To manually check the result of these two first steps we first only look at Myanmar data, removing all minerals.

```{r,out.width="50%",fig.cap="Biplot for myanmar sample. First row, dots collored by region, second row collored by site"}
mya=isotopes[isotopes$Country=="Myanmar",]

minerals <- c( "SEALIP/MY/BAW/1", "SEALIP/MY/BAW/2", "SEALIP/MY/BAW/3", "SEALIP/MY/BWD/1", "SEALIP/MY/BWD/2", "SEALIP/MY/KAW/1", "SEALIP/MY/KAW/2", "SEALIP/MY/KAW/3", "SEALIP/MY/MIN/1", "SEALIP/MY/MIN/2", "SEALIP/MY/MIN/3", "SEALIP/MY/NTL/1", "SEALIP/MY/NTL/2", "SEALIP/MY/NTL/3", "SEALIP/MY/PDHT/1", "SEALIP/MY/PDHT/2", "SEALIP/MY/PDHT/3", "SEALIP/MY/ST/1", "SEALIP/MY/ST/2", "SEALIP/MY/ST/3", "SEALIP/MY/TKM/1", "SEALIP/MY/TKM/2", "SEALIP/MY/TKM/3", "SEALIP/MY/LPDT/1", "SEALIP/MY/YTCM/1")
nomin=mya[!(mya$LABEL.analytical %in% minerals),]


nomin$Region[nomin$Region==""]="Unkown"
regcol=rainbow(length(unique(nomin$Region)),alpha=.8)
names(regcol)=unique(nomin$Region)
plot(nomin[,ratioiso[3:2]],bg=regcol[nomin$Region],pch=20+as.numeric(as.factor(nomin$Region)))
plot(nomin[,ratioiso[c(3,1)]],bg=regcol[nomin$Region],pch=20+as.numeric(as.factor(nomin$Region)))
val=legend("bottomright",pt.bg=regcol,,pch=20+(1:length(unique(nomin$Region))),legend=names(regcol),title="eite",bty="n",ncol=4,cex=1)

sitecol=rainbow(length(unique(nomin$Site.abreviation)),alpha=.8)
names(sitecol)=unique(nomin$Site.abreviation)
plot(nomin[,ratioiso[3:2]],col=sitecol[nomin$Site.abreviation],pch=as.numeric(as.factor(nomin$Site.abreviation)))
plot(nomin[,ratioiso[c(3,1)]],col=sitecol[nomin$Site.abreviation],pch=as.numeric(as.factor(nomin$Site.abreviation)))
val=legend("bottomright",col=sitecol,,pch=1:length(unique(nomin$Site.abreviation)),legend=names(sitecol),title="Site",bty="n",ncol=4,cex=1)
```

Once we have the subset we want to analyse, we can create the consistency matrix:
```{r}
nomin.consmat=createRatioMatrix(nomin,ratioiso,threshold) #this function return a scale between 0 and 3: 3 all isotopics ratios matches ; 0 no one matches. For no we want full matches; so we ch
dimnames(nomin.consmat)=list(nomin$LABELS.shorts,nomin$LABELS.shorts)
nomin.consmat[nomin.consmat<3]=0
nomin.consmat[nomin.consmat==3]=1

```

This can be directly adapted as a graph:
```{r}

nomin.graph = graphFromAdj(nomin.consmat,label=nomin$LABELS.shorts)
plot(nomin.graph)
```

Following Radivojevic & Grujic we use community detection to create 'families' of consistent artefact. The original paper used Louvain clustering algorithm:

```{r}
library(igraph)
nomin.clus=cluster_louvain(nomin.graph)
V(nomin.graph)$color=nomin.clus$membership
plot(nomin.graph)
```

On problem I noticed with this methods is that it get in troubles when lot of component are not connected. It groups together groups that should not. It is not shown in the previous example as we select the best data to cluster but it appeared sometime during my analysis, thus I decided to use Leiden clustering algorithm:

```{r}
nomin.clus=cluster_leiden(nomin.graph,objective_function="modularity")
V(nomin.graph)$color=nomin.clus$membership
plot(nomin.graph)
#val=legend("bottomright",pt.bg=categorical_pal(nomin.clus$nb_clusters),pch=21,legend=paste("Famille",1:nomin.clus$nb_clusters),title="Families",bty="n",ncol=2,pt.cex=2)
```

Let's visualise the families on the initial Bi-lpots:

```{r families-biplot,out.width="45%",fig.cap="Bi-plot for isotopic ratios, sample are color given the families detected via the method described previously"}

sitecol=rep(adjustcolor(categorical_pal(nomin.clus$nb_clusters),.6),4)
plot(nomin[,ratioiso[3:2]],bg=sitecol[nomin.clus$membership],pch=21,cex=2)
text(nomin[,ratioiso[c(3,2)]],nomin$LABELS.shorts,,cex=.6,pos=1)
plot(nomin[,ratioiso[c(3,1)]],bg=sitecol[nomin.clus$membership],pch=21,cex=2)
text(nomin[,ratioiso[c(3,1)]],nomin$LABELS.shorts,,cex=.6,pos=1)
val=legend("bottomright",pt.bg=sitecol,pch=21,legend=paste("Famille",1:nomin.clus$nb_clusters),title="Families",bty="n",ncol=2,pt.cex=2)
```

We could spend  more time exploring those families, what is there centroid (mean isotope composition, etc...), but following the initial idea, we know went to compare geographical area through the lens of this grouping.

#### Myanmar Sites Network

As when creating the consistency matrix, there are plenty of ways to measure the similarity/distance of composition of each site give the families of artefact it has. For now, and following Radivojevic & Grujic, we will count how many artefact two sites have in common. Let's do that and see the result for two big sites : HL28 and NGO

```{r countTypesExample,out.width="20%",fig.height=8,fig.width=4,fig.cap="Composition of families of artefacts for the two site HL28 and NGO"  }
HL=which(nomin$Site.abreviation=="HL28")
NG=which(nomin$Site.abreviation=="NGO")


countBothSites=cbind(HL=table(factor(nomin.clus$membership[HL],levels=1:8)),
                    NG=table(factor(nomin.clus$membership[NG],levels=1:8)))

barplot(countBothSites,col=categorical_pal(nomin.clus$nb_clusters),legend=T,args.legend=list(title="Familiy"),ylab="#artefact")

```
```{r,results="asis"}
rownames(countBothSites)=paste("Family",1:8)
knitr::kable(countBothSites)
```

We see that HL has 10 artefact from family 1 , 1 from family 2 and some other from other famili. NGO has 9 from family 1, and 1 from faimily 2. This means there is a total of 9 + 1 = 10 matches.
oThis is given by this function:

```{r}
RGmatch(nomin.clus$membership[HL],nomin.clus$membership[NG])
```


Applying this to the current dataset (myanmar without mineral)  gives the following sites matrix:

```{r}
nomin.sitemat=createAreaAdjMat(nomin,nomin$Site.abreviation,nomin.clus$membership,match=RGmatch)
```

Where we can check that:


```{r,result='asis'}
nomin.sitemat["NGO","HL28"]
```

is ten

This, again can be directly translated into a graph to extract communities of sites that share similar traits:

```{r}
nomin.sitegraph = graphFromAdj(nomin.sitemat,label=rownames(nomin.sitemat))
nomin.sitecluster=cluster_leiden(nomin.sitegraph,objective_function = "modularity")
V(nomin.sitegraph)$color=nomin.sitecluster$membership
E(nomin.sitegraph)$width=E(nomin.sitegraph)$weight
plot(nomin.sitegraph)
```

There is lot of ways to play with the distance we computed, we can take the log of it ; to reduce the difference between small and very big number:
```{r}

nomin.sitegraph = graphFromAdj(log(nomin.sitemat+1),label=rownames(nomin.sitemat))
nomin.sitecluster=cluster_leiden(nomin.sitegraph,objective_function = "modularity")
V(nomin.sitegraph)$color=nomin.sitecluster$membership
E(nomin.sitegraph)$width=E(nomin.sitegraph)$weight
plot(nomin.sitegraph)
```

We can normalise by the number of artefact per site ; to render for the number of artefact per site . This connect together the site that are exactly the same.

```{r}
sampsize=table(nomin$Site.abreviation)
nomin.sitematnormalized=apply(nomin.sitemat,2,function(i)(i/sampsize[names(i)]))
nomin.sitegraph = graphFromAdj(nomin.sitematnormalized,label=rownames(nomin.sitematnormalized))
nomin.sitecluster=cluster_leiden(nomin.sitegraph,objective_function = "modularity")
V(nomin.sitegraph)$color=nomin.sitecluster$membership
E(nomin.sitegraph)$width=E(nomin.sitegraph)$weight
plot(nomin.sitegraph)
```

Another metric, my feeling (the match normalized by the total multiplied by the log of the total)

```{r}
nomin.sitematNM=createAreaAdjMat(nomin,nomin$Site.abreviation,nomin.clus$membership,match=RGmatchNormalised)
nomin.sitegraphNM= graphFromAdj(nomin.sitematNM,label=rownames(nomin.sitematNM))
nomin.siteclusterNM=cluster_leiden(nomin.sitegraphNM,objective_function = "modularity")
V(nomin.sitegraphNM)$color=nomin.siteclusterNM$membership
E(nomin.sitegraphNM)$width=E(nomin.sitegraphNM)$weight
plot(nomin.sitegraphNM)
```

We can plot these communities of sites on the map, I will use R&G match and mine side by side


```{r,out.width="50%",fig.cap="Left community detected by original R&G match, right using a normalized version of it"}
nomin.sitemat=createAreaAdjMat(nomin,nomin$Site.abreviation,nomin.clus$membership,match=RGmatch)
nomin.sitegraph = graphFromAdj(nomin.sitemat,label=rownames(nomin.sitemat))
nomin.sitecluster=cluster_leiden(nomin.sitegraph,objective_function = "modularity")
V(nomin.sitegraph)$color=nomin.sitecluster$membership
E(nomin.sitegraph)$width=E(nomin.sitegraph)$weight

nomin.sitegraph=colorEdges(nomin.sitegraph)
nomin.sitegraph.coords=coordForGraph(nomin.sitegraph,nomin,"Site.abreviation")
plot(nomin.sitegraph.coords,ann=F,axes=F,type="n")
plot(nomin.sitegraph,layout=nomin.sitegraph.coords,add=T,rescale=F)
graph.sp=st_as_sf(as.data.frame(nomin.sitegraph.coords),coords=1:2,crs=st_crs(seashapefile))
plot(st_crop(seashapefile,graph.sp),add=T)

nomin.sitegraphNM=colorEdges(nomin.sitegraphNM)
nomin.sitegraphNM.coords=coordForGraph(nomin.sitegraphNM,nomin,"Site.abreviation")
plot(nomin.sitegraphNM.coords,ann=F,axes=F,type="n")
plot(nomin.sitegraphNM,layout=nomin.sitegraphNM.coords,add=T,rescale=F)
graph.sp=st_as_sf(as.data.frame(nomin.sitegraphNM.coords),coords=1:2,crs=st_crs(seashapefile))
plot(st_crop(seashapefile,graph.sp),add=T)

```

Lets look community by comunity


```{r allgraph,out.width="48%",fig.cap="Graph community by community for Myanmar withou minerals"}

for(gp in which(lengths(nomin.sitecluster,use.names=F)>1)){
    sub=subgraph(nomin.sitegraph,nomin.sitecluster[[gp]])
    sub=set_graph_attr(sub,"layout",nomin.sitegraph.coords[V(sub)$name,])
    try({
        plot(nomin.sitegraph.coords,ann=F,axes=F,type="n")
        plot(sub,add=T,rescale=F)
        graph.sp=st_as_sf(as.data.frame(nomin.sitegraph.coords),coords=1:2,crs=st_crs(seashapefile))
        plot(st_crop(seashapefile,graph.sp),add=T)
    })
}

```


## Generalisation to full Myanmar and dataset


```{r,out.width="60%",fig.cap="Network of consistency between all sample from  mynamar "}
mya.consmat=createRatioMatrix(mya,ratioiso,threshold) 
dimnames(mya.consmat)=list(mya$LABELS.shorts,mya$LABELS.shorts)
mya.consmat[mya.consmat<3]=0
mya.consmat[mya.consmat==3]=1
mya.graph = graphFromAdj(mya.consmat,label=mya$LABELS.shorts)
mya.clus=cluster_leiden(mya.graph,objective_function="modularity")
V(mya.graph)$color=mya.clus$membership
plot(mya.graph,main="Network of artefact consistency")
```

```{r,out.width="48%",fig.cap="bi-plot for all Myanmar data. Top row show the whole dataset;bottom row the axis are limited to certain values."}

sitecol=rep(adjustcolor(categorical_pal(mya.clus$nb_clusters),.6),8)
plot(mya[,ratioiso[3:2]],bg=sitecol[mya.clus$membership],pch=21,cex=2, main="Full biplot for all Myanmar")
plot(mya[,ratioiso[c(3,1)]],bg=sitecol[mya.clus$membership],pch=21,cex=2)
val=legend("bottomright",pt.bg=sitecol,pch=21,legend=paste("Famille",1:mya.clus$nb_clusters),title="Families",bty="n",ncol=5,pt.cex=1.5,cex=.6)

ratiolim=list(c(17.7,19.3),c(15.45,15.85),c(37.5,40))
names(ratiolim)=rev(ratioiso)
plot(mya[,ratioiso[3:2]],bg=sitecol[mya.clus$membership],pch=21,cex=2,xlim=ratiolim[[1]],ylim=ratiolim[[2] ], main="bi-plot for all Myanmar, limited range")
text(mya[,ratioiso[c(3,2)]],mya$LABELS.shorts,,cex=.6,pos=1)
plot(mya[,ratioiso[c(3,1)]],bg=sitecol[mya.clus$membership],pch=21,cex=2,xlim=ratiolim[[1]],ylim=ratiolim[[3] ])
text(mya[,ratioiso[c(3,1)]],mya$LABELS.shorts,,cex=.6,pos=1)
val=legend("bottomright",pt.bg=sitecol,pch=21,legend=paste("Famille",1:mya.clus$nb_clusters),title="Families",bty="n",ncol=5,pt.cex=1.5,cex=.6)

```

```{r,out.width="48%",fig.cap="Community detected in all  mynamar data "}
mya.sitemat=createAreaAdjMat(mya,mya$Site.abreviation,mya.clus$membership,match=RGmatch)
mya.sitegraph = graphFromAdj(mya.sitemat,label=rownames(mya.sitemat))
mya.sitecluster=cluster_leiden(mya.sitegraph,objective_function = "modularity")
V(mya.sitegraph)$color=mya.sitecluster$membership
E(mya.sitegraph)$width=E(mya.sitegraph)$weight
mya.sitegraph=colorEdges(mya.sitegraph)
plot(mya.sitegraph,main="Sites network")

mya.sitegraph.coords=coordForGraph(mya.sitegraph,mya,"Site.abreviation")
plot(mya.sitegraph.coords,ann=F,axes=F,type="n")
plot(mya.sitegraph,layout=mya.sitegraph.coords,add=T,rescale=F)
invalidcoords=apply(apply(mya.sitegraph.coords,2,is.na),1,any)
graph.sp=st_as_sf(as.data.frame(mya.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
plot(st_crop(seashapefile,graph.sp),add=T)
```

```{r,out.width="45%",fig.cap="network of communities on the map; separating communities"}
for(gp in which(lengths(mya.sitecluster,use.names=F)>1)){
    sub=subgraph(mya.sitegraph,mya.sitecluster[[gp]])
    sub=set_graph_attr(sub,"layout",mya.sitegraph.coords[V(sub)$name,])
    try({
    plot(mya.sitegraph.coords,ann=F,axes=F,type="n")
    plot(sub,add=T,rescale=F)
    invalidcoords=apply(apply(mya.sitegraph.coords,2,is.na),1,any)
    graph.sp=st_as_sf(as.data.frame(mya.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
    plot(st_crop(seashapefile,graph.sp),add=T)
    })
}
```

Version with my match function ( _ie_ we use the same groups of consistenc artefacts, we use a different way to link sites given their composition of artefact's families")

```{r plot-whole-myanmar-mymatch}
mya.sitemat=createAreaAdjMat(mya,mya$Site.abreviation,mya.clus$membership,match=RGmatchNormalised)
mya.sitegraph = graphFromAdj(mya.sitemat,label=rownames(mya.sitemat))
mya.sitecluster=cluster_leiden(mya.sitegraph,objective_function = "modularity")
V(mya.sitegraph)$color=mya.sitecluster$membership
E(mya.sitegraph)$width=E(mya.sitegraph)$weight
mya.sitegraph=colorEdges(mya.sitegraph)
plot(mya.sitegraph,main="row collored Sites Network")
mya.sitegraph.coords=coordForGraph(mya.sitegraph,mya,"Site.abreviation")
mya.sitegraph=set_graph_attr(mya.sitegraph,"layout",mya.sitegraph.coords)
plot(mya.sitegraph.coords,ann=F,axes=F,type="n")
plot(mya.sitegraph,layout=mya.sitegraph.coords,add=T,rescale=F)
invalidcoords=apply(apply(mya.sitegraph.coords,2,is.na),1,any)
graph.sp=st_as_sf(as.data.frame(mya.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
plot(st_crop(seashapefile,graph.sp),add=T)
```

Checking  communities separately

```{r,out.width="45%"}
for(gp in which(lengths(mya.sitecluster,use.names=F)>1)){
    sub=subgraph(mya.sitegraph,mya.sitecluster[[gp]])
    sub=set_graph_attr(sub,"layout",mya.sitegraph.coords[V(sub)$name,])
    try({
    plot(mya.sitegraph.coords,ann=F,axes=F,type="n")
    plot(sub,add=T,rescale=F)
    invalidcoords=apply(apply(mya.sitegraph.coords,2,is.na),1,any)
    graph.sp=st_as_sf(as.data.frame(mya.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
    plot(st_crop(seashapefile,graph.sp),add=T)
    })
}

```



# Whole dataset: 

```{r plot-whole-dataset,out.width="50%",fig.cap="Consistency graph, and biplot for the whole dataset"}

isotopes.consmat=createRatioMatrix(isotopes,ratioiso,threshold) 
dimnames(isotopes.consmat)=list(isotopes$LABELS.shorts,isotopes$LABELS.shorts)
isotopes.consmat[isotopes.consmat<3]=0
isotopes.consmat[isotopes.consmat==3]=1
isotopes.graph = graphFromAdj(isotopes.consmat,label=isotopes$LABELS.shorts)
isotopes.clus=cluster_leiden(isotopes.graph,objective_function="modularity")
V(isotopes.graph)$color=isotopes.clus$membership
plot(isotopes.graph,main="Netowrk of consistency between artefacts")

sitecol=rep(adjustcolor(categorical_pal(isotopes.clus$nb_clusters),.6),100)
plot(isotopes[,ratioiso[3:2]],bg=sitecol[isotopes.clus$membership],pch=21,cex=2,main="Full biplot for whole dataset")
plot(isotopes[,ratioiso[c(3,1)]],bg=sitecol[isotopes.clus$membership],pch=21,cex=2)

ratiolim=list(c(17.7,19.3),c(15.45,15.85),c(37.5,40))
names(ratiolim)=rev(ratioiso)
plot(isotopes[,ratioiso[3:2]],bg=sitecol[isotopes.clus$membership],pch=21,cex=2,xlim=ratiolim[[1]],ylim=ratiolim[[2] ],main="Limited range biplot for whole dataset")
plot(isotopes[,ratioiso[c(3,1)]],bg=sitecol[isotopes.clus$membership],pch=21,cex=2,xlim=ratiolim[[1]],ylim=ratiolim[[3] ])


isotopes.sitemat=createAreaAdjMat(isotopes,isotopes$Site.abreviation,isotopes.clus$membership,match=RGmatch)
isotopes.sitegraph = graphFromAdj(isotopes.sitemat,label=rownames(isotopes.sitemat))
isotopes.sitecluster=cluster_leiden(isotopes.sitegraph,objective_function = "modularity")
V(isotopes.sitegraph)$color=isotopes.sitecluster$membership
E(isotopes.sitegraph)$width=E(isotopes.sitegraph)$weight
isotopes.sitegraph=colorEdges(isotopes.sitegraph)
plot(isotopes.sitegraph,main="networkd of sites")
isotopes.sitegraph.coords=coordForGraph(isotopes.sitegraph,isotopes,"Site.abreviation")
plot(isotopes.sitegraph.coords,ann=F,axes=F,type="n")
plot(isotopes.sitegraph,layout=isotopes.sitegraph.coords,add=T,rescale=F)
invalidcoords=apply(apply(isotopes.sitegraph.coords,2,is.na),1,any)
graph.sp=st_as_sf(as.data.frame(isotopes.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
plot(st_crop(seashapefile,graph.sp),add=T)
```

Check community by communities

```{r,out.width="50%",fig.cap="communities detected for the whole dataset"}

for(gp in which(lengths(isotopes.sitecluster,use.names=F)>1)){
    sub=subgraph(isotopes.sitegraph,isotopes.sitecluster[[gp]])
    sub=set_graph_attr(sub,"layout",isotopes.sitegraph.coords[V(sub)$name,])
    try({
    plot(isotopes.sitegraph.coords,ann=F,axes=F,type="n")
    plot(sub,add=T,rescale=F)
    invalidcoords=apply(apply(isotopes.sitegraph.coords,2,is.na),1,any)
    graph.sp=st_as_sf(as.data.frame(isotopes.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
    plot(st_crop(seashapefile,graph.sp),add=T)
    })
}

```

Version with my match function ( _ie_ we use the same groups of consistenc artefacts, we use a different way to link sites given their composition of artefact's families")

```{r plot-whole-deateset-mymatch}
isotopes.sitemat=createAreaAdjMat(isotopes,isotopes$Site.abreviation,isotopes.clus$membership,match=RGmatchNormalised)
isotopes.sitegraph = graphFromAdj(isotopes.sitemat,label=rownames(isotopes.sitemat))
isotopes.sitecluster=cluster_leiden(isotopes.sitegraph,objective_function = "modularity")
V(isotopes.sitegraph)$color=isotopes.sitecluster$membership
E(isotopes.sitegraph)$width=E(isotopes.sitegraph)$weight
isotopes.sitegraph=colorEdges(isotopes.sitegraph)
plot(isotopes.sitegraph,main="networkd of sites")
isotopes.sitegraph.coords=coordForGraph(isotopes.sitegraph,isotopes,"Site.abreviation")
plot(isotopes.sitegraph.coords,ann=F,axes=F,type="n")
plot(isotopes.sitegraph,layout=isotopes.sitegraph.coords,add=T,rescale=F)
invalidcoords=apply(apply(isotopes.sitegraph.coords,2,is.na),1,any)
graph.sp=st_as_sf(as.data.frame(isotopes.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
plot(st_crop(seashapefile,graph.sp),add=T)
```

Check community by communities

```{r,out.width="50%",fig.cap="communities detected for the whole dataset using normalized metric"}
for(gp in which(lengths(isotopes.sitecluster,use.names=F)>1)){
    sub=subgraph(isotopes.sitegraph,isotopes.sitecluster[[gp]])
    sub=set_graph_attr(sub,"layout",isotopes.sitegraph.coords[V(sub)$name,])
    try({
    plot(isotopes.sitegraph.coords,ann=F,axes=F,type="n")
    plot(sub,add=T,rescale=F)
    invalidcoords=apply(apply(isotopes.sitegraph.coords,2,is.na),1,any)
    graph.sp=st_as_sf(as.data.frame(isotopes.sitegraph.coords[!invalidcoords,]),coords=1:2,crs=st_crs(seashapefile))
    plot(st_crop(seashapefile,graph.sp),add=T)
    })
}

```




# Next steps:

There are multiples axes we can follow, to check, explore and make things better here and there. There are still _some_ problems with the consistency check. By using the method of adding link when the three ratios are consistent, we get some "chains" of consistency the can spread very far on the 206.204 axis when we have lot of data point. Check the grey family on the bottom of the graph in the section 3 for example. 

Clustering techniques likes DBScan  may help to cut using points density (cf notes about it down), but something still needs to be done on this side as it may be the source of multiple 'inconsistent consistencies'.  

Regardless this problem, and moving to the next stage (the network of sites),  the next question would be : how, once we get a satisfactory consistency between isotopes composition, do we group/link the sites - geographical area?

To answer that ; something that could facilitate our task (and that also help to know when we are roughly satisfied with the consistency grouping) is to think about what higher level questions the analysis we ultimately answer. The map you showed Oli could be a first start, but we need to precise things: do we want to find if the network of road correspond to specific communities (ie specific set of artefacts families that 'travel' together?), do we want to be able to define 'remarkable' geographical regions  (where remarkable could be: more diverse than the other regions, with connection to more, that only contain specific communities,....). 

Defining such questions (which I guess will also depend on the journal/paper we have in sight) will then greatly help to choose the right methods (do we want to correct for missing sample, for the intensity of the connection of site, for the geographical distance between two sites,...).











# A few notes:

### On notation:

We should decided and stick to certain words ; for now I see at least two terms that are confusion:
Ratio:  used for the isotopic ratio and the distance between these ratio
Sample: used to speak about the lab sample, that have been tested, which is equivalent to "artefact";  but which, in statistic, also mean "a random selection of element within a set". I use "element" here and used it, in the sense "element within a set" but as you both rightly remarked element is used for elemental data, so we can't used. I don't really like `artefact`, but so far it's the option i started to use.


### On DBScan: 

DBSCAN is based on density of point in a space to cluster. The "only" think that prevented me to use it is to define a distance metric that I can use to cut. This distance need to be logarithmic (distance on big value should be conted the same that distance on the lower side). Is a logarithm of the euclidian distance between the point on the 3 axes good? that what's I should check

